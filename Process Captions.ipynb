{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Process Captions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Processing Captions**"
      ],
      "metadata": {
        "id": "HNZ_h-k1zbc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNE7TNePqetY",
        "outputId": "f157802b-4a76-456f-dd00-823cdef6ed68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import gensim\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating final CSV with image names and corresponding Captions**\n",
        "\n"
      ],
      "metadata": {
        "id": "egjxr6YFxxqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_final_csv(caption_path):\n",
        "  with open(caption_path) as f:\n",
        "    content = f.readlines()\n",
        "  content = [x.strip() for x in content]\n",
        "  content = [x.split(\".jpg,\") for x in content]\n",
        "  df = pd.DataFrame(content[1:])\n",
        "  df.columns = ['Images_name','captions']\n",
        "  df = df.drop_duplicates(subset=['Images_name'])\n",
        "  df['Images_name']= df['Images_name'].astype(str)+'.jpg'\n",
        "  df['captions']= df['captions'].str.lower()\n",
        "  df = df.reset_index(drop=True)\n",
        "  df.head()\n",
        "  df.to_csv('/CSVs/flickr8K_final.csv',index=False)\n",
        "\n",
        "caption_path = '/Captions/captions.txt'\n",
        "create_final_csv(caption_path)"
      ],
      "metadata": {
        "id": "HALWw3KNrJlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning captions, removing puncutaions, Stop Words, Sapces etc...**"
      ],
      "metadata": {
        "id": "w3MpqWG0yN8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_tokenize_comments_for_image(comment):\n",
        "    stop_words = ['a', 'and', 'of', 'to']\n",
        "    punctuation = r\"\"\"!\"#$%&'()*+,./:;<=>?@[\\]^_`…’{|}~\"\"\"\n",
        "    captions_without_punctuation = [s.translate(str.maketrans(' ', ' ', punctuation)) for s in comment]\n",
        "    sentences = []\n",
        "\n",
        "    for clean_caption in captions_without_punctuation:\n",
        "        clean_caption = re.sub(r\"-(?:(?<!\\b[0-9]{4}-)|(?![0-9]{2}(?:[0-9]{2})?\\b))\", ' ', clean_caption)  # replace with space\n",
        "\n",
        "        temp_tokens = word_tokenize(str(clean_caption).lower())\n",
        "        tokens = [t for t in temp_tokens if t not in stop_words]\n",
        "        sentences.append(tokens)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "p8awFMZOtC-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating vectors of captions using Word2Vec**"
      ],
      "metadata": {
        "id": "ZYLg9yY3yros"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_vectors_for_single_comment(word2vec_model, cleaned_comments, image_names):\n",
        "    vectorized_list = []\n",
        "    image_list = []\n",
        "\n",
        "    for comments, image in zip(cleaned_comments, image_names):\n",
        "        result_array = np.empty((0, 300))\n",
        "        for word in comments:\n",
        "            try:\n",
        "                w = [word2vec_model[word]]\n",
        "                result_array = np.append(result_array, w, axis=0)\n",
        "            except KeyError:\n",
        "                print(word)\n",
        "                result_array = np.append(result_array, [word2vec_model[random.choice(word2vec_model.index2entity)]], axis=0)\n",
        "\n",
        "        vectorized_list.append(np.mean(result_array, axis=0).astype('float32'))\n",
        "        image_list.append(image)\n",
        "\n",
        "    return image_list, np.array(vectorized_list)"
      ],
      "metadata": {
        "id": "PNX9EDA3tN4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating embeddings and saving to pickle**"
      ],
      "metadata": {
        "id": "xeMpA3Gwy8CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentence_embeddings():\n",
        "    df = pd.read_csv('CSVs/flickr8K_final.csv')\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format('/word2vec_pretrained_model/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "    cleaned_captions = clean_and_tokenize_comments_for_image(df['captions'].values)\n",
        "    image_names = df['Images_name'].values\n",
        "    print('Done tokenizing....')\n",
        "    i, c = create_feature_vectors_for_single_comment(model, cleaned_captions, image_names)\n",
        "    word_vector_dict = dict(zip(i, c))\n",
        "    pickle.dump(word_vector_dict, open('/Pickles/flickr8k_embeddings' + \".p\", \"wb\"))\n",
        "    print('Done')\n",
        "\n",
        "create_sentence_embeddings()"
      ],
      "metadata": {
        "id": "VdE4nZYPunKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xcWG5cMFecMB"
      }
    }
  ]
}